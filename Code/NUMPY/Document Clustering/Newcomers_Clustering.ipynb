{"cells":[{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"e22303ff5cef428e9111f212029cfa5c","deepnote_cell_height":82,"deepnote_cell_type":"markdown"},"source":"# Import Library"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"3cf35b29f1d34125a2c5c11d95f1efb0","deepnote_cell_height":52.390625,"deepnote_cell_type":"markdown"},"source":"This code is for importing library what I need."},{"cell_type":"code","metadata":{"tags":[],"cell_id":"86d0fa5921b846a897d6df8854bf6067","source_hash":"986a20c9","execution_start":1683276093973,"execution_millis":34584,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"!pip install numpy\n!pip install pandas==1.5.3\n!pip install nltk\n!pip install beautifulsoup4\n!pip install mpld3\n!pip install -U scikit-learn\n!pip install joblib\n!pip install gensim\n!pip install pyLDAvis==3.4.0","execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: numpy in /shared-libs/python3.9/py/lib/python3.9/site-packages (1.23.4)\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mCollecting pandas==1.5.3\n  Downloading pandas-1.5.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from pandas==1.5.3) (2.8.2)\nRequirement already satisfied: numpy>=1.20.3 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from pandas==1.5.3) (1.23.4)\nRequirement already satisfied: pytz>=2020.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from pandas==1.5.3) (2022.5)\nRequirement already satisfied: six>=1.5 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.16.0)\nInstalling collected packages: pandas\n  Attempting uninstall: pandas\n    Found existing installation: pandas 1.2.5\n    Not uninstalling pandas at /shared-libs/python3.9/py/lib/python3.9/site-packages, outside environment /root/venv\n    Can't uninstall 'pandas'. No files were found to uninstall.\nSuccessfully installed pandas-1.5.3\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: nltk in /shared-libs/python3.9/py/lib/python3.9/site-packages (3.7)\nRequirement already satisfied: click in /shared-libs/python3.9/py/lib/python3.9/site-packages (from nltk) (8.1.3)\nRequirement already satisfied: joblib in /shared-libs/python3.9/py/lib/python3.9/site-packages (from nltk) (1.2.0)\nRequirement already satisfied: regex>=2021.8.3 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from nltk) (2022.9.13)\nRequirement already satisfied: tqdm in /shared-libs/python3.9/py/lib/python3.9/site-packages (from nltk) (4.64.1)\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: beautifulsoup4 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (4.11.1)\nRequirement already satisfied: soupsieve>1.2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from beautifulsoup4) (2.3.2.post1)\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mCollecting mpld3\n  Downloading mpld3-0.5.9-py3-none-any.whl (201 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.2/201.2 KB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: jinja2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from mpld3) (2.11.3)\nRequirement already satisfied: matplotlib in /shared-libs/python3.9/py/lib/python3.9/site-packages (from mpld3) (3.6.0)\nRequirement already satisfied: MarkupSafe>=0.23 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from jinja2->mpld3) (2.0.0)\nRequirement already satisfied: cycler>=0.10 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib->mpld3) (0.11.0)\nRequirement already satisfied: pillow>=6.2.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib->mpld3) (9.2.0)\nRequirement already satisfied: contourpy>=1.0.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib->mpld3) (1.0.5)\nRequirement already satisfied: numpy>=1.19 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib->mpld3) (1.23.4)\nRequirement already satisfied: pyparsing>=2.2.1 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from matplotlib->mpld3) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from matplotlib->mpld3) (2.8.2)\nRequirement already satisfied: kiwisolver>=1.0.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib->mpld3) (1.4.4)\nRequirement already satisfied: fonttools>=4.22.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib->mpld3) (4.37.4)\nRequirement already satisfied: packaging>=20.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from matplotlib->mpld3) (21.3)\nRequirement already satisfied: six>=1.5 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->mpld3) (1.16.0)\nInstalling collected packages: mpld3\nSuccessfully installed mpld3-0.5.9\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: scikit-learn in /shared-libs/python3.9/py/lib/python3.9/site-packages (1.1.2)\nCollecting scikit-learn\n  Downloading scikit_learn-1.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from scikit-learn) (1.2.0)\nRequirement already satisfied: numpy>=1.17.3 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from scikit-learn) (1.23.4)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from scikit-learn) (3.1.0)\nRequirement already satisfied: scipy>=1.3.2 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from scikit-learn) (1.9.3)\nInstalling collected packages: scikit-learn\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.1.2\n    Not uninstalling scikit-learn at /shared-libs/python3.9/py/lib/python3.9/site-packages, outside environment /root/venv\n    Can't uninstall 'scikit-learn'. No files were found to uninstall.\nSuccessfully installed scikit-learn-1.2.2\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: joblib in /shared-libs/python3.9/py/lib/python3.9/site-packages (1.2.0)\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mCollecting gensim\n  Downloading gensim-4.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.18.5 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from gensim) (1.23.4)\nRequirement already satisfied: smart-open>=1.8.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from gensim) (5.2.1)\nRequirement already satisfied: scipy>=1.7.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from gensim) (1.9.3)\nInstalling collected packages: gensim\nSuccessfully installed gensim-4.3.1\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mCollecting pyLDAvis==3.4.0\n  Downloading pyLDAvis-3.4.0-py3-none-any.whl (2.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting numexpr\n  Downloading numexpr-2.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (380 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.7/380.7 KB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: gensim in /root/venv/lib/python3.9/site-packages (from pyLDAvis==3.4.0) (4.3.1)\nRequirement already satisfied: jinja2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from pyLDAvis==3.4.0) (2.11.3)\nRequirement already satisfied: scikit-learn>=1.0.0 in /root/venv/lib/python3.9/site-packages (from pyLDAvis==3.4.0) (1.2.2)\nRequirement already satisfied: pandas>=1.3.4 in /root/venv/lib/python3.9/site-packages (from pyLDAvis==3.4.0) (1.5.3)\nCollecting funcy\n  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\nRequirement already satisfied: numpy>=1.22.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from pyLDAvis==3.4.0) (1.23.4)\nRequirement already satisfied: joblib>=1.2.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from pyLDAvis==3.4.0) (1.2.0)\nRequirement already satisfied: setuptools in /root/venv/lib/python3.9/site-packages (from pyLDAvis==3.4.0) (58.1.0)\nRequirement already satisfied: scipy in /shared-libs/python3.9/py/lib/python3.9/site-packages (from pyLDAvis==3.4.0) (1.9.3)\nRequirement already satisfied: python-dateutil>=2.8.1 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from pandas>=1.3.4->pyLDAvis==3.4.0) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from pandas>=1.3.4->pyLDAvis==3.4.0) (2022.5)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from scikit-learn>=1.0.0->pyLDAvis==3.4.0) (3.1.0)\nRequirement already satisfied: smart-open>=1.8.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from gensim->pyLDAvis==3.4.0) (5.2.1)\nRequirement already satisfied: MarkupSafe>=0.23 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from jinja2->pyLDAvis==3.4.0) (2.0.0)\nRequirement already satisfied: six>=1.5 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas>=1.3.4->pyLDAvis==3.4.0) (1.16.0)\nInstalling collected packages: funcy, numexpr, pyLDAvis\nSuccessfully installed funcy-2.0 numexpr-2.8.4 pyLDAvis-3.4.0\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"50aa8987d5cf47ebaa73ec8805a56ea3","source_hash":"dee12b86","execution_start":1683276128562,"execution_millis":11010,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"import numpy as np\nimport pandas as pd\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nfrom bs4 import BeautifulSoup\nimport re\nimport os \nimport codecs\nfrom sklearn import feature_extraction\nimport mpld3\nfrom glob import glob \nimport json\n\nimport numpy as np\nimport pandas as pd\nfrom pprint import pprint\n\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\n\nimport spacy\n\nimport pyLDAvis\nimport pyLDAvis.gensim_models as gensimvis\npyLDAvis.enable_notebook()\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n","execution_count":2,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n/shared-libs/python3.9/py/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n2023-05-05 08:42:14.530273: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-05-05 08:42:14.676075: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2023-05-05 08:42:14.676120: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2023-05-05 08:42:14.705143: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-05-05 08:42:16.288683: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2023-05-05 08:42:16.288773: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2023-05-05 08:42:16.288783: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n2023-05-05 08:42:18.342754: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n2023-05-05 08:42:18.342793: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n2023-05-05 08:42:18.342818: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (p-4121e9a6-526b-4eeb-ae8d-65c5504611ef): /proc/driver/nvidia/version does not exist\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"0721cdbd5b864df49bfbd07aec010166","deepnote_cell_height":82,"deepnote_cell_type":"markdown"},"source":"# Document Clustering"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"7ed36fbb25b3413c80e68838e1f071fb","deepnote_cell_height":62,"deepnote_cell_type":"markdown"},"source":"### Stopwords, stemming, and tokenizing"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"eee075c17cdd4d988fce68e7bb117846","deepnote_cell_height":74.78125,"deepnote_cell_type":"markdown"},"source":"This section is focused on defining some functions to manipulate the titles/bodies. First, I load NLTK's list of English stop words. Stop words are words like \"a\", \"the\", or \"in\" which don't convey significant meaning. I'm sure there are much better explanations of this out there."},{"cell_type":"code","metadata":{"tags":[],"cell_id":"0e477f436899425995f9ba8733836a62","source_hash":"d94fdbed","execution_start":1683276139555,"execution_millis":17,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# load nltk's English stopwords as variable called 'stopwords'\nnew_stopwords = [\"use\", \"abl\", \"would\", \"like\", \"made\", \"happen\", \"id\", \"n\\'t\", \"expect\", \"1\", \"useline\", \"arn\", \"aw\", \"int2byt\", \"py35\", \"python\", \"kw\", \"def\", \"six\", \"six.mov\", \"\\'s\", \"ns\", \"-r\", \"int2byte_alt\", \"python3\", \"six.add_metaclass\", \"setup.pi\", \"import_modul\", \"r\", \"x\", \"__name__\", \"0m\", \"1m\" ,\"\\'m\", \"'m\", \"flask\", \"pkgutil.get_load\", \"ani\", \"-g\", \"_configtest.c\", \"unknown._\", \"f\", \"\\'d\", \"|sin\", \"|cos\", \"h\", \"numpy.float64\", \"|squar\", \"|absolut\", \"|sqrt\", \"|reciproc\", \"b\", \"-fwrapv\", \"defin\", \"x86_64-linux-gnu-gcc\", \"-fstack-protector-strong\", \"numpi\", \"µs\", \"ms\", \"//projects.scipy.org/numpy/ticket/709\", \"v\", \"datetime.datetime.now\", \"side=\\'left\", \"bench_avx\", \"v_shape\", \"np.searchsort\", \"__init__.pi\", \"array_lik\", \"ss\", \"cdk\", \"aws-cdk\", \"kwarg\", \"name\", \"iam\", \"aws-cdk/cor\", \"aws-cdk/cfnspec\", \"callabl\", \"arg\", \"l\", \"0x485b9e\", \"0x0\", \"cjw296\", \"numpy.isnan\", \"xxx\", \"\\\\n\", \"__init__\"]\nstopwords = nltk.corpus.stopwords.words('english')\nstopwords.extend(new_stopwords)\nprint (stopwords)","execution_count":3,"outputs":[{"name":"stdout","text":"['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'use', 'abl', 'would', 'like', 'made', 'happen', 'id', \"n't\", 'expect', '1', 'useline', 'arn', 'aw', 'int2byt', 'py35', 'python', 'kw', 'def', 'six', 'six.mov', \"'s\", 'ns', '-r', 'int2byte_alt', 'python3', 'six.add_metaclass', 'setup.pi', 'import_modul', 'r', 'x', '__name__', '0m', '1m', \"'m\", \"'m\", 'flask', 'pkgutil.get_load', 'ani', '-g', '_configtest.c', 'unknown._', 'f', \"'d\", '|sin', '|cos', 'h', 'numpy.float64', '|squar', '|absolut', '|sqrt', '|reciproc', 'b', '-fwrapv', 'defin', 'x86_64-linux-gnu-gcc', '-fstack-protector-strong', 'numpi', 'µs', 'ms', '//projects.scipy.org/numpy/ticket/709', 'v', 'datetime.datetime.now', \"side='left\", 'bench_avx', 'v_shape', 'np.searchsort', '__init__.pi', 'array_lik', 'ss', 'cdk', 'aws-cdk', 'kwarg', 'name', 'iam', 'aws-cdk/cor', 'aws-cdk/cfnspec', 'callabl', 'arg', 'l', '0x485b9e', '0x0', 'cjw296', 'numpy.isnan', 'xxx', '\\\\n', '__init__']\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"657ff13417a5472e982bd99a1ee2d933","deepnote_cell_height":52.390625,"deepnote_cell_type":"markdown"},"source":"Next I import the Snowball Stemmer which is actually part of NLTK. Stemming is just the process of breaking a word down into its root."},{"cell_type":"code","metadata":{"tags":[],"cell_id":"e5804e9d9d6b4e139d55b2578c4c41cc","source_hash":"28e5ddc3","execution_start":1683276139561,"execution_millis":12,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# load nltk's SnowballStemmer as variabled 'stemmer'\nfrom nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer(\"english\")","execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"c1c470a86ef64125b166516dabeb0ba7","deepnote_cell_height":119.5625,"deepnote_cell_type":"markdown"},"source":"Below I define two functions: \ntokenize_and_stem: tokenizes (splits the titles into a list of its respective words (or tokens) and also stems each token tokenize_only: tokenizes the titles only I use both these functions to create a dictionary which becomes important in case I want to use stems for an algorithm, but later convert stems back to their full words for presentation purposes. Guess what, I do want to do that!"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"864056948bc34dba9f07635eb193d6e8","source_hash":"26e07d18","execution_start":1683276139574,"execution_millis":37,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n\ndef tokenize_and_stem(text):\n    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n    filtered_tokens = []\n    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n    for token in tokens:\n        if re.search('[a-zA-Z]', token):\n            filtered_tokens.append(token)\n    stems = [stemmer.stem(t) for t in filtered_tokens]\n    return stems\n\n\ndef tokenize_only(text):\n    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n    filtered_tokens = []\n    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n    for token in tokens:\n        if re.search('[a-zA-Z]', token):\n            filtered_tokens.append(token)\n    return filtered_tokens","execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"5ed7bc5d2f1b400a95eb86c1a7c1d690","deepnote_cell_height":62,"deepnote_cell_type":"markdown"},"source":"### Latent Dirichlet Allocation"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"e1dfb2448dad4bad8595831f1a6b70ba","deepnote_cell_height":74.78125,"deepnote_cell_type":"markdown"},"source":"For my implementaiton of LDA, I use the Gensim package. I'm going to preprocess the titles a bit differently here, and first I define a function to remove any proper noun."},{"cell_type":"code","metadata":{"tags":[],"cell_id":"4ae03849d21848ff948b6c42244b0438","source_hash":"fed3c6de","execution_start":1683276139610,"execution_millis":1,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#strip any proper names from a text...unfortunately right now this is yanking the first word from a sentence too.\nimport string\ndef strip_proppers(text):\n    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent) if word.islower()]\n    return \"\".join([\" \"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in tokens]).strip()","execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"da8047bf5d934a798272571e21d3df75","deepnote_cell_height":52.390625,"deepnote_cell_type":"markdown"},"source":"Here I run the actual text processing (removing of proper nouns, tokenization, removal of stop words)"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"260576298aad43d9b730bcff64b90cab","source_hash":"d514ebce","execution_start":1683276139609,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#strip any proper nouns (NNP) or plural proper nouns (NNPS) from a text\nfrom nltk.tag import pos_tag\n\ndef strip_proppers_POS(text):\n    tagged = pos_tag(text.split()) #use NLTK's part of speech tagger\n    non_propernouns = [word for word,pos in tagged if pos != 'NNP' and pos != 'NNPS']\n    return non_propernouns","execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"c3bd0bf8203148629c7b4a4f1949ecbc","deepnote_cell_height":52.390625,"deepnote_cell_type":"markdown"},"source":"Here I run the actual text processing (removing of proper nouns, tokenization, removal of stop words)"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"12754008e032462cb4e6f2723a703863","source_hash":"41ed286e","execution_start":1683276139610,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#Latent Dirichlet Allocation implementation with Gensim\n\nfrom gensim import corpora, models, similarities ","execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"cbb8acfaad4741fda0f397239c34d41d","deepnote_cell_height":70,"deepnote_cell_type":"markdown"},"source":"## Title and Body Newcomers Scenario"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"d1d8793404c64f8f877177629615080f","deepnote_cell_height":62,"deepnote_cell_type":"markdown"},"source":"### Stopwords, stemming, and tokenizing"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"3a08f1df5fcb418aa075c5171b79ce14","deepnote_cell_height":52.390625,"deepnote_cell_type":"markdown"},"source":"Access, split and compute data from newcomers body and title txt files."},{"cell_type":"code","metadata":{"tags":[],"cell_id":"2e1c1c41e29344a3a4469b5a8d1b5ad9","source_hash":"d457e577","execution_start":1683276139613,"execution_millis":12,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#import data title\ntitles = open('data_title_newcomers.txt').read().split('\\BREAKHERE')\n\n#ensures that only the first 1825 are read in\ntitles = titles[1:1826]\n\n#import data body\nbodies = open('data_body_newcomers.txt').read().split('\\BREAKHERE')\n\n#ensures that only the first 1825 are read in\nbodies = bodies[1:1826]\n\nprint(str(len(titles)) + ' titles')\nprint(str(len(bodies)) + ' bodies')","execution_count":9,"outputs":[{"name":"stdout","text":"8 titles\n8 bodies\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"76d4605658514b45b80bb8f16d44ea92","source_hash":"2d45d9c","execution_start":1659334512087,"execution_millis":52,"deepnote_cell_height":52.390625,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"markdown"},"source":"Enter newcomers body and title data into the list"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"169d17d09c9842cdb22b25b1fc834101","source_hash":"7e93ebd1","execution_start":1683276139659,"execution_millis":4,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# generates index for each item\nlist_titles = []\n\nfor text in titles:\n    text = BeautifulSoup(text, 'html.parser').getText()\n    #strips html formatting and converts to unicode\n    list_titles.append(text)\n\ntitles = list_titles\n\n# generates index for each item\nlist_bodies = []\n\nfor text in bodies:\n    text = BeautifulSoup(text, 'html.parser').getText()\n    #strips html formatting and converts to unicode\n    list_bodies.append(text)\n\nbodies = list_bodies\n\nbodies_titles = []\n\nfor i in range(len(bodies)):\n    item = bodies[i] + titles[i]\n    bodies_titles.append(item)","execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"69390870211041bcaa7b1b6827928810","deepnote_cell_height":52.390625,"deepnote_cell_type":"markdown"},"source":"Check newcomers body and title data."},{"cell_type":"code","metadata":{"tags":[],"cell_id":"af366a2293d64907a4d9a28a839b96ed","source_hash":"40c1d167","execution_start":1683276139658,"execution_millis":4,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"pd.DataFrame(bodies_titles)","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":1,"row_count":8,"columns":[{"name":0,"dtype":"object","stats":{"unique_count":8,"nan_count":0,"categories":[{"name":"improve code quality\n\n\nSTY: unify imports in __init__.py","count":1},{"name":"The signature is\n```python\nnp.searchsorted(a, v, side='left', sorter=None)\n```\n\nIt is difficult to remember which role is played by `a` and which by `v`.\n\nI propose renaming the args to\n```python\nnp.searchsorted(haystack, needles, side='left', sorter=None)\n```\n(not perfect, I know, but better than `a` and `v`)\n\nThis would improve my user experience in jupyter notebook, hitting shift-tab after `np.searchsorted(`.\nI'm sure other editors/development environments would benefit too.\n\nHappy to do the PR if there's interest\n\nsearchsorted arg names are bad","count":1},{"name":"6 others","count":6}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"0":"improve code quality\n\n\nSTY: unify imports in __init__.py","_deepnote_index_column":"0"},{"0":"The signature is\n```python\nnp.searchsorted(a, v, side='left', sorter=None)\n```\n\nIt is difficult to remember which role is played by `a` and which by `v`.\n\nI propose renaming the args to\n```python\nnp.searchsorted(haystack, needles, side='left', sorter=None)\n```\n(not perfect, I know, but better than `a` and `v`)\n\nThis would improve my user experience in jupyter notebook, hitting shift-tab after `np.searchsorted(`.\nI'm sure other editors/development environments would benefit too.\n\nHappy to do the PR if there's interest\n\nsearchsorted arg names are bad","_deepnote_index_column":"1"},{"0":"The Type checker inspection in PyCharm (2020.1.1) gives the following warning when trying to initialize a ``random.BitGenerator`` with an ``array_like`` of ints or a ``random.SeedSequence``: ``Expected type 'Optional[int]', got 'SeedSequence' instead``. It tries to interpret the types given for the ``seed`` argument in the docstring:\n\n```python\nseed : {None, int, array_like[ints], SeedSequence}, optional\n```\n\nHowever, it only recognizes ``None`` and ``int``.\n\n### Reproducing code example:\n\n```python\nfrom numpy.random import SeedSequence, SFC64\nss = SeedSequence()\nrng = SFC64(ss)\nrng = SFC64([1, 2, 3])\n```\n\n### Numpy/Python version information:\n\n1.18.4 3.8.2 (tags/v3.8.2:7b3ab59, Feb 25 2020, 23:03:10) [MSC v.1916 64 bit (AMD64)]\nPyCharm Type checker warning when correctly initializing a random.BitGenerator","_deepnote_index_column":"2"},{"0":"I want to test the performance optimization of a function such as np.multiply，After reading the benchmark ReadMe file, I get the following command(multiply function lies in bench_avx suite):\n`python runtests.py --bench-compare HEAD bench_avx`\nIt takes about 4 minutes to complete the test, so much time wasted in testing other functions such as sin/cos/exp, because they also lies in bench_avx, Is there any way that I could run a single multiply benchmark test without adding new suite?\n\nBTW, I spend some time in benchmark setup, because the ReadMe file didn't tell developers that asv and virtualenv should be preinstalled by pip before benchmarking, maybe doc should be improved.BenchMark: How to run a single benchmark test?","_deepnote_index_column":"3"},{"0":"The error message produced by setting value of mismatching shape to array is incorrect.\n\nHere is an example.\n\n```python\n>>> a = numpy.zeros((4, 4, 4))\n>>> ind = numpy.array([[1,0], [2, 3]])\n>>> v_shape = (2,2,4,4)\n>>> v = numpy.arange(np.prod(v_shape)).reshape(v_shape)\n>>> a[:, ind, :] = v\n\nValueError: shape mismatch: value array of shape (2,2,4,4) could not be broadcast to indexing result of shape (2,2,4,4)\n```\n\nThe shape that is requested by this example is `(4, 2, 2, 4)`. Therefore, error message should say \n\n```python\nValueError: shape mismatch: value array of shape (2,2,4,4) could not be broadcast to indexing result of shape (4,2,2,4)\n```\n__setitem__ shape mismatch error is incorrect","_deepnote_index_column":"4"},{"0":"The docstring of the first argument of loadtxt currently states\n\n```\n    fname : file or str\n        File, filename, or generator to read.  If the filename extension is\n        ``.gz`` or ``.bz2``, the file is first decompressed. Note that\n        generators should return byte strings for Python 3k.\n```\n\nIn fact the generator can yield strings:\n\n```\nIn [1]: np.loadtxt(iter([\"1 2 3\", \"4 5 6\"])) # or io.StringIO\nOut[1]: \narray([[ 1.,  2.,  3.],\n       [ 4.,  5.,  6.]])\n```\n\nNot making a patch yet because we may as well clarify encoding issues at the same time... or not? Something like \"If the file or generator is yielding unicode strings, they will be decoded as...\"\nUpdating the docstring of loadtxt","_deepnote_index_column":"5"},{"0":"_Original ticket http://projects.scipy.org/numpy/ticket/709 on 2008-03-20 by trac user cjw296, assigned to unknown._\n\nI'm faily sure that:\n\nnumpy.isnan(datetime.datetime.now())\n\n...should just return False and not raise an exception.\n\nrationale: anything that is not nan should just return False, not raise TypeErrors.\nisnan should not raise TypeErrors (Trac #709)","_deepnote_index_column":"6"},{"0":"","_deepnote_index_column":"7"}]},"text/plain":"                                                   0\n0  improve code quality\\n\\n\\nSTY: unify imports i...\n1  The signature is\\n```python\\nnp.searchsorted(a...\n2  The Type checker inspection in PyCharm (2020.1...\n3  I want to test the performance optimization of...\n4  The error message produced by setting value of...\n5  The docstring of the first argument of loadtxt...\n6  _Original ticket http://projects.scipy.org/num...\n7                                                   ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>improve code quality\\n\\n\\nSTY: unify imports i...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The signature is\\n```python\\nnp.searchsorted(a...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The Type checker inspection in PyCharm (2020.1...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I want to test the performance optimization of...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The error message produced by setting value of...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>The docstring of the first argument of loadtxt...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>_Original ticket http://projects.scipy.org/num...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"94ed5eda65454545a105a47b9448653e","deepnote_cell_height":74.78125,"deepnote_cell_type":"markdown"},"source":"Below I use my stemming/tokenizing and tokenizing functions to iterate over the list of bodies and titles to create two vocabularies: one stemmed and one only tokenized."},{"cell_type":"code","metadata":{"tags":[],"cell_id":"1fdfeaf8261343029fb6ddb2a16354ba","source_hash":"70599251","execution_start":1683276139666,"execution_millis":44,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"totalvocab_stemmed = []\ntotalvocab_tokenized = []\nfor i in bodies_titles:\n    allwords_stemmed = tokenize_and_stem(i)\n    totalvocab_stemmed.extend(allwords_stemmed)\n    \n    allwords_tokenized = tokenize_only(i)\n    totalvocab_tokenized.extend(allwords_tokenized)","execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"6538b6eb0ad6492392b062475c5b9d70","deepnote_cell_height":119.5625,"deepnote_cell_type":"markdown"},"source":"Using these two lists, I create a pandas DataFrame with the stemmed vocabulary as the index and the tokenized words as the column. The benefit of this is it provides an efficient way to look up a stem and return a full token. The downside here is that stems to tokens are one to many: the stem 'run' could be associated with 'ran', 'runs', 'running', etc. For my purposes this is fine--I'm perfectly happy returning the first token associated with the stem I need to look up."},{"cell_type":"code","metadata":{"tags":[],"cell_id":"b8b2dd896d744628b8ec00ffc2214a8c","source_hash":"bff4c2ad","execution_start":1683276139711,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)","execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"5ddf7d0a78da46fd8ad48d0297777591","deepnote_cell_height":52.390625,"deepnote_cell_type":"markdown"},"source":"Check newcomers bodies and titles data vocab frame."},{"cell_type":"code","metadata":{"tags":[],"cell_id":"451c0e4565a949dc8bc897d7d09db63d","source_hash":"d60c0211","execution_start":1683276139742,"execution_millis":6,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"vocab_frame","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":1,"row_count":529,"columns":[{"name":"words","dtype":"object","stats":{"unique_count":279,"nan_count":0,"categories":[{"name":"the","count":23},{"name":"is","count":13},{"name":"277 others","count":493}]}},{"name":"_deepnote_index_column","dtype":"object"}],"rows":[{"words":"improve","_deepnote_index_column":"improv"},{"words":"code","_deepnote_index_column":"code"},{"words":"quality","_deepnote_index_column":"qualiti"},{"words":"sty","_deepnote_index_column":"sti"},{"words":"unify","_deepnote_index_column":"unifi"},{"words":"imports","_deepnote_index_column":"import"},{"words":"in","_deepnote_index_column":"in"},{"words":"__init__.py","_deepnote_index_column":"__init__.pi"},{"words":"the","_deepnote_index_column":"the"},{"words":"signature","_deepnote_index_column":"signatur"}]},"text/plain":"                words\nimprov        improve\ncode             code\nqualiti       quality\nsti               sty\nunifi           unify\n...               ...\nshould         should\nnot               not\nrais            raise\ntypeerror  typeerrors\ntrac             trac\n\n[529 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>words</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>improv</th>\n      <td>improve</td>\n    </tr>\n    <tr>\n      <th>code</th>\n      <td>code</td>\n    </tr>\n    <tr>\n      <th>qualiti</th>\n      <td>quality</td>\n    </tr>\n    <tr>\n      <th>sti</th>\n      <td>sty</td>\n    </tr>\n    <tr>\n      <th>unifi</th>\n      <td>unify</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>should</th>\n      <td>should</td>\n    </tr>\n    <tr>\n      <th>not</th>\n      <td>not</td>\n    </tr>\n    <tr>\n      <th>rais</th>\n      <td>raise</td>\n    </tr>\n    <tr>\n      <th>typeerror</th>\n      <td>typeerrors</td>\n    </tr>\n    <tr>\n      <th>trac</th>\n      <td>trac</td>\n    </tr>\n  </tbody>\n</table>\n<p>529 rows × 1 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"5da8f5ff6f36426eb621a0e923dfd5c9","deepnote_cell_height":52.390625,"deepnote_cell_type":"markdown"},"source":"Check the shape of items in vocab_frame"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"5a0e8789c0b4407e91dfdb71da7c6fba","source_hash":"48880c26","execution_start":1683276139743,"execution_millis":6,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"print('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')","execution_count":15,"outputs":[{"name":"stdout","text":"there are 529 items in vocab_frame\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"1e1bf883d586400181eba4852f4536d4","deepnote_cell_height":62,"deepnote_cell_type":"markdown"},"source":"### Latent Dirichlet Allocation"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"25fd1e256bbe4b438fc2416072db3de5","deepnote_cell_height":52.390625,"deepnote_cell_type":"markdown"},"source":"Here I run the actual text processing (removing of proper nouns, tokenization, removal of stop words)"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"86e9f4d3b351405b89105ccb7851718d","source_hash":"54b3cc39","execution_start":1683276139748,"execution_millis":12,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#Latent Dirichlet Allocation implementation with Gensim\n\n#remove proper names\npreprocess = [strip_proppers(doc) for doc in bodies_titles]\n\n%time tokenized_text = [tokenize_and_stem(text) for text in preprocess]\n\n%time texts = [[word for word in text if word not in stopwords] for text in tokenized_text]","execution_count":16,"outputs":[{"name":"stdout","text":"CPU times: user 5.73 ms, sys: 0 ns, total: 5.73 ms\nWall time: 5.72 ms\nCPU times: user 854 µs, sys: 0 ns, total: 854 µs\nWall time: 855 µs\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"a207f4ec12824054a6dd685d56e0288e","source_hash":"7cc1e639","execution_start":1683276139759,"execution_millis":19,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#print(len([word for word in texts[0] if word not in stopwords]))\nprint(len(texts[0]))","execution_count":17,"outputs":[{"name":"stdout","text":"5\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"ca5917b0f9af4d11b75541e391c1ac5b","deepnote_cell_height":52.390625,"deepnote_cell_type":"markdown"},"source":"Below are some Gensim specific conversions; I also filter out extreme words (see inline comment)"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"15b88553691044b49357e276c562ceb0","source_hash":"655ea696","execution_start":1683276139786,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#create a Gensim dictionary from the texts\ndictionary = corpora.Dictionary(texts)\n\n#remove extremes (similar to the min/max df step used when creating the tf-idf matrix)\ndictionary.filter_extremes(no_below=1, no_above=0.8)\n\n#convert the dictionary to a bag of words corpus for reference\ncorpus = [dictionary.doc2bow(text) for text in texts]","execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"a25e2e9e82d64f21a876803304237f27","source_hash":"da0313b4","execution_start":1683276139792,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"len(corpus)","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"8"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"04622c6e050948ababb28dab0bdf11b3","deepnote_cell_height":97.171875,"deepnote_cell_type":"markdown"},"source":"The actual model runs below. I took 1825 passes to ensure convergence, but you can see that it took my machine 4 minutes to run. My chunksize is larger than the corpus so basically all synopses are used per pass. I should optimize this, and Gensim has the capacity to run in parallel. I'll likely explore this further as I use the implementation on larger corpora."},{"cell_type":"code","metadata":{"tags":[],"cell_id":"9a67f0ef27104aeaac86508a735e398e","source_hash":"76dbf521","execution_start":1683276139799,"execution_millis":352,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# %time lda = models.LdaModel(corpus, num_topics=10, id2word=dictionary, update_every=5, chunksize=10000, passes=100)\n%time lda = models.LdaModel(corpus, num_topics=5, id2word=dictionary, update_every=5, chunksize=10000, passes=100)","execution_count":20,"outputs":[{"name":"stdout","text":"CPU times: user 331 ms, sys: 4.02 ms, total: 335 ms\nWall time: 342 ms\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"6ebb465915bc422c88ad90d8914bcc64","deepnote_cell_height":52.390625,"deepnote_cell_type":"markdown"},"source":"Each topic has a set of words that defines it, along with a certain probability."},{"cell_type":"code","metadata":{"tags":[],"cell_id":"a1735245563143799f0113d22a788140","source_hash":"2dd021d5","execution_start":1683276140193,"execution_millis":4,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# lda.show_topics()\n\npprint(lda.print_topics())","execution_count":21,"outputs":[{"name":"stdout","text":"[(0,\n  '0.052*\"generat\" + 0.039*\"file\" + 0.039*\"string\" + 0.027*\"docstr\" + '\n  '0.027*\"first\" + 0.027*\"loadtxt\" + 0.027*\"yield\" + 0.027*\"filenam\" + '\n  '0.015*\"argument\" + 0.015*\"read\"'),\n (1,\n  '0.035*\"improv\" + 0.019*\"propos\" + 0.019*\"renam\" + 0.019*\"notebook\" + '\n  '0.019*\"perfect\" + 0.019*\"play\" + 0.019*\"needl\" + 0.019*\"rememb\" + '\n  '0.019*\"shift-tab\" + 0.019*\"searchsort\"'),\n (2,\n  '0.076*\"shape\" + 0.043*\"int\" + 0.035*\"mismatch\" + 0.026*\"array\" + '\n  '0.026*\"exampl\" + 0.026*\"valu\" + 0.026*\"error\" + 0.018*\"could\" + '\n  '0.018*\"messag\" + 0.018*\"result\"'),\n (3,\n  '0.056*\"benchmark\" + 0.056*\"test\" + 0.034*\"function\" + 0.024*\"becaus\" + '\n  '0.024*\"time\" + 0.024*\"singl\" + 0.024*\"suit\" + 0.024*\"lie\" + 0.024*\"run\" + '\n  '0.024*\"multipli\"'),\n (4,\n  '0.065*\"rais\" + 0.045*\"return\" + 0.024*\"user\" + 0.024*\"sure\" + '\n  '0.024*\"assign\" + 0.024*\"except\" + 0.024*\"ticket\" + 0.024*\"anyth\" + '\n  '0.024*\"rational\" + 0.024*\"nan\"')]\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"721c0796b9cd4184b74254612d50438a","allow_embed":"output","source_hash":"ecc8adcd","execution_start":1683276140194,"execution_millis":316,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"pyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim_models.prepare(lda, corpus, dictionary)\nvis","execution_count":22,"outputs":[{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\ntopic                                                \n2      0.185152 -0.041427       1        1  35.779821\n3     -0.120866 -0.118747       2        1  24.837407\n0     -0.055419  0.117424       3        1  19.980616\n1     -0.002906  0.026209       4        1  12.362445\n4     -0.005961  0.016541       5        1   7.039712, topic_info=          Term      Freq     Total Category  logprob  loglift\n119      shape  7.000000  7.000000  Default  30.0000  30.0000\n158       rais  1.000000  1.000000  Default  29.0000  29.0000\n64   benchmark  3.000000  3.000000  Default  28.0000  28.0000\n93        test  3.000000  3.000000  Default  27.0000  27.0000\n135    generat  2.000000  2.000000  Default  26.0000  26.0000\n..         ...       ...       ...      ...      ...      ...\n61      becaus  0.070397  2.484906   Topic5  -5.5094  -0.9102\n94        time  0.070397  2.484906   Topic5  -5.5094  -0.9102\n31         bit  0.070397  1.292911   Topic5  -5.5094  -0.2569\n33     correct  0.070397  1.292911   Topic5  -5.5094  -0.2569\n37        give  0.070397  1.292911   Topic5  -5.5094  -0.2569\n\n[204 rows x 6 columns], token_table=      Topic      Freq         Term\nterm                              \n121       3  0.853801         .bz2\n122       3  0.853801          .gz\n100       1  0.773448      .reshap\n123       3  0.853801           3k\n101       1  0.773448  __setitem__\n...     ...       ...          ...\n120       1  0.727570         valu\n57        1  0.989661         warn\n148       3  0.853801         well\n149       3  0.853801          yet\n150       3  0.562579        yield\n\n[144 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 4, 1, 2, 5])","text/html":"\n<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n\n\n<div id=\"ldavis_el801398433851731526551792193\" style=\"background-color:white;\"></div>\n<script type=\"text/javascript\">\n\nvar ldavis_el801398433851731526551792193_data = {\"mdsDat\": {\"x\": [0.1851524404915394, -0.12086634368993927, -0.05541869288101291, -0.0029061754039359142, -0.005961228516651277], \"y\": [-0.041426679461851135, -0.11874691013471772, 0.11742395452094247, 0.026208502623894108, 0.01654113245173233], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [35.77982067527595, 24.837406503805486, 19.98061623510001, 12.362444889179178, 7.039711696639377]}, \"tinfo\": {\"Term\": [\"shape\", \"rais\", \"benchmark\", \"test\", \"generat\", \"return\", \"string\", \"file\", \"improv\", \"int\", \"function\", \"sure\", \"user\", \"mismatch\", \"loadtxt\", \"first\", \"yield\", \"filenam\", \"anyth\", \"assign\", \"except\", \"faili\", \"http\", \"isnan\", \"nan\", \"ticket\", \"trac\", \"rational\", \"docstr\", \"lie\", \"shape\", \"int\", \"mismatch\", \"error\", \"valu\", \"exampl\", \"broadcast\", \"incorrect\", \"ind\", \"index\", \"messag\", \"result\", \"checker\", \"initi\", \"rng\", \"seed\", \"tri\", \"type\", \"warn\", \"array\", \".reshap\", \"__setitem__\", \"np.prod\", \"numpy.arang\", \"numpy.array\", \"numpy.zero\", \"produc\", \"request\", \"say\", \"set\", \"could\", \"code\", \"import\", \"benchmark\", \"test\", \"function\", \"lie\", \"multipli\", \"run\", \"singl\", \"suit\", \"ad\", \"also\", \"asv\", \"befor\", \"bench-compar\", \"command\", \"complet\", \"develop\", \"doc\", \"get\", \"mayb\", \"minut\", \"much\", \"new\", \"optim\", \"perform\", \"pip\", \"preinstal\", \"runtests.pi\", \"setup\", \"sin/cos/exp\", \"spend\", \"becaus\", \"time\", \"file\", \"follow\", \"generat\", \"string\", \"filenam\", \"first\", \"loadtxt\", \"yield\", \".bz2\", \".gz\", \"3k\", \"byte\", \"clarifi\", \"current\", \"decod\", \"decompress\", \"encod\", \"extens\", \"fact\", \"fname\", \"issu\", \"iter\", \"make\", \"may\", \"np.loadtxt\", \"patch\", \"state\", \"str\", \"unicod\", \"well\", \"yet\", \"docstr\", \"file\", \"argument\", \"read\", \"improv\", \"bad\", \"benefit\", \"better\", \"difficult\", \"editors/develop\", \"environ\", \"experi\", \"haystack\", \"hit\", \"interest\", \"jupyt\", \"know\", \"needl\", \"notebook\", \"perfect\", \"play\", \"propos\", \"rememb\", \"renam\", \"role\", \"searchsort\", \"shift-tab\", \"signatur\", \"qualiti\", \"unifi\", \"sure\", \"user\", \"code\", \"import\", \"rais\", \"anyth\", \"assign\", \"except\", \"faili\", \"http\", \"isnan\", \"nan\", \"rational\", \"ticket\", \"trac\", \"return\", \"sure\", \"user\", \"qualiti\", \"unifi\", \"bad\", \"benefit\", \"better\", \"difficult\", \"editors/develop\", \"environ\", \"experi\", \"haystack\", \"hit\", \"interest\", \"jupyt\", \"know\", \"needl\", \"notebook\", \"code\", \"import\", \"improv\", \"argument\", \"follow\", \"docstr\", \"array\", \"could\", \"read\", \"becaus\", \"time\", \"bit\", \"correct\", \"give\"], \"Freq\": [7.0, 1.0, 3.0, 3.0, 2.0, 1.0, 2.0, 3.0, 1.0, 4.0, 2.0, 1.0, 1.0, 3.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 1.0, 6.6974064728669225, 3.7854838874196135, 3.057503734898059, 2.3295227593093832, 2.3295227593093832, 2.3295227593093832, 1.601541289880434, 1.601541289880434, 1.601541289880434, 1.601541289880434, 1.601541289880434, 1.601541289880434, 1.6015411252670098, 1.6015411252670098, 1.6015411252670098, 1.6015411252670098, 1.6015411252670098, 1.6015411252670098, 1.6015411252670098, 2.329612309012228, 0.8735586681575146, 0.8735586681575146, 0.8735586681575146, 0.8735586681575146, 0.8735586681575146, 0.8735586681575146, 0.8735586681575146, 0.8735586681575146, 0.8735586681575146, 0.8735586681575146, 1.6015625250121748, 0.8738022137188358, 0.8738022137188358, 3.415548381516332, 3.415548381516332, 2.1018722001174677, 1.4450333095259957, 1.4450333095259957, 1.4450333095259957, 1.4450333095259957, 1.4450333095259957, 0.7881937904479205, 0.7881937904479205, 0.7881937904479205, 0.7881937904479205, 0.7881937904479205, 0.7881937904479205, 0.7881937904479205, 0.7881937904479205, 0.7881937904479205, 0.7881937904479205, 0.7881937904479205, 0.7881937904479205, 0.7881937904479205, 0.7881937904479205, 0.7881937904479205, 0.7881937904479205, 0.7881937904479205, 0.7881937904479205, 0.7881937904479205, 0.7881937904479205, 0.7881937904479205, 0.7881937904479205, 1.4451174124605006, 1.4451174124605006, 1.444948063888576, 0.7882584674328795, 2.5464293716424833, 1.9401345979736657, 1.3338396404538622, 1.3338396404538622, 1.3338396404538622, 1.3338396404538622, 0.7275435798281413, 0.7275435798281413, 0.7275435798281413, 0.7275435798281413, 0.7275435798281413, 0.7275435798281413, 0.7275435798281413, 0.7275435798281413, 0.7275435798281413, 0.7275435798281413, 0.7275435798281413, 0.7275435798281413, 0.7275435798281413, 0.7275435798281413, 0.7275435798281413, 0.7275435798281413, 0.7275435798281413, 0.7275435798281413, 0.7275435798281413, 0.7275435798281413, 0.7275435798281413, 0.7275435798281413, 0.7275435798281413, 1.3340661448688893, 1.9402142054507006, 0.7276158792284736, 0.7275470729968796, 1.0595896138715368, 0.5779567612882983, 0.5779567612882983, 0.5779567612882983, 0.5779567612882983, 0.5779567612882983, 0.5779567612882983, 0.5779567612882983, 0.5779567612882983, 0.5779567612882983, 0.5779567612882983, 0.5779567612882983, 0.5779567612882983, 0.5779567612882983, 0.5779567612882983, 0.5779567612882983, 0.5779567612882983, 0.5779567612882983, 0.5779567612882983, 0.5779567612882983, 0.5779567612882983, 0.5779567612882983, 0.5779567612882983, 0.5779567612882983, 0.5779467510566855, 0.5779467510566855, 0.5779486848514288, 0.5779486848514288, 0.5777879523824636, 0.5777879523824636, 1.126341749852067, 0.422375046962042, 0.422375046962042, 0.422375046962042, 0.422375046962042, 0.422375046962042, 0.422375046962042, 0.422375046962042, 0.422375046962042, 0.422375046962042, 0.422375046962042, 0.7744434164827655, 0.422382042735129, 0.422382042735129, 0.07039987344621691, 0.07039987344621691, 0.0703972095465113, 0.0703972095465113, 0.0703972095465113, 0.0703972095465113, 0.0703972095465113, 0.0703972095465113, 0.0703972095465113, 0.0703972095465113, 0.0703972095465113, 0.0703972095465113, 0.0703972095465113, 0.0703972095465113, 0.0703972095465113, 0.0703972095465113, 0.07040060217258015, 0.07040060217258015, 0.07039828644213697, 0.07039773584888474, 0.07039770346104637, 0.0703976872671272, 0.07039752532793536, 0.07039751723097577, 0.07039738767962231, 0.07039733909786476, 0.07039733909786476, 0.07039733100090517, 0.07039733100090517, 0.07039733100090517], \"Total\": [7.0, 1.0, 3.0, 3.0, 2.0, 1.0, 2.0, 3.0, 1.0, 4.0, 2.0, 1.0, 1.0, 3.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 1.0, 7.116758883540113, 4.204836712411637, 3.47685621696522, 2.748875303750842, 2.748875303750842, 2.7488754347260116, 2.0208939324130495, 2.0208939324130495, 2.0208939324130495, 2.0208939324130495, 2.0208939324130495, 2.0208939324130495, 2.0208941784026444, 2.0208941784026444, 2.0208941784026444, 2.0208941784026444, 2.0208941784026444, 2.0208941784026444, 2.0208941784026444, 3.355176214697364, 1.2929116465054518, 1.2929116465054518, 1.2929116465054518, 1.2929116465054518, 1.2929116465054518, 1.2929116465054518, 1.2929116465054518, 1.2929116465054518, 1.2929116465054518, 1.2929116465054518, 2.677724049398869, 1.7746254235988093, 1.7746254235988093, 3.849128501760472, 3.8491285120488112, 2.5354523644566718, 1.8786135260572236, 1.8786135260572236, 1.8786135260572236, 1.8786135260572236, 1.8786135260572236, 1.2217742191409708, 1.2217742191409708, 1.2217742191409708, 1.2217742191409708, 1.2217742191409708, 1.2217742191409708, 1.2217742191409708, 1.2217742191409708, 1.2217742191409708, 1.2217742191409708, 1.2217742191409708, 1.2217742191409708, 1.2217742191409708, 1.2217742191409708, 1.2217742191409708, 1.2217742191409708, 1.2217742191409708, 1.2217742191409708, 1.2217742191409708, 1.2217742191409708, 1.2217742191409708, 1.2217742191409708, 2.484906483814364, 2.484906483814364, 3.697484224206988, 1.9497322138979705, 2.9901182919378435, 2.383823533475525, 1.777528655512971, 1.777528655512971, 1.777528655512971, 1.777528655512971, 1.1712327986338253, 1.1712327986338253, 1.1712327986338253, 1.1712327986338253, 1.1712327986338253, 1.1712327986338253, 1.1712327986338253, 1.1712327986338253, 1.1712327986338253, 1.1712327986338253, 1.1712327986338253, 1.1712327986338253, 1.1712327986338253, 1.1712327986338253, 1.1712327986338253, 1.1712327986338253, 1.1712327986338253, 1.1712327986338253, 1.1712327986338253, 1.1712327986338253, 1.1712327986338253, 1.1712327986338253, 1.1712327986338253, 2.5054483598908996, 3.697484224206988, 1.8991836115595182, 1.828060360711527, 1.5282154050493832, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.0465787194676377, 1.0465787194676377, 1.3985571462314206, 1.3985571462314206, 1.7746254235988093, 1.7746254235988093, 1.6208940505032683, 0.9169278405295149, 0.9169278405295149, 0.9169278405295149, 0.9169278405295149, 0.9169278405295149, 0.9169278405295149, 0.9169278405295149, 0.9169278405295149, 0.9169278405295149, 0.9169278405295149, 1.8751364551867238, 1.3985571462314206, 1.3985571462314206, 1.0465787194676377, 1.0465787194676377, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.0465783316720558, 1.7746254235988093, 1.7746254235988093, 1.5282154050493832, 1.8991836115595182, 1.9497322138979705, 2.5054483598908996, 3.355176214697364, 2.677724049398869, 1.828060360711527, 2.484906483814364, 2.484906483814364, 1.2929113548580196, 1.2929113548580196, 1.2929113548580196], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.5799, -3.1504, -3.364, -3.6359, -3.6359, -3.6359, -4.0106, -4.0106, -4.0106, -4.0106, -4.0106, -4.0106, -4.0106, -4.0106, -4.0106, -4.0106, -4.0106, -4.0106, -4.0106, -3.6359, -4.6168, -4.6168, -4.6168, -4.6168, -4.6168, -4.6168, -4.6168, -4.6168, -4.6168, -4.6168, -4.0106, -4.6165, -4.6165, -2.8882, -2.8882, -3.3737, -3.7484, -3.7484, -3.7484, -3.7484, -3.7484, -4.3546, -4.3546, -4.3546, -4.3546, -4.3546, -4.3546, -4.3546, -4.3546, -4.3546, -4.3546, -4.3546, -4.3546, -4.3546, -4.3546, -4.3546, -4.3546, -4.3546, -4.3546, -4.3546, -4.3546, -4.3546, -4.3546, -3.7484, -3.7484, -3.7485, -4.3545, -2.9643, -3.2362, -3.6109, -3.6109, -3.6109, -3.6109, -4.2171, -4.2171, -4.2171, -4.2171, -4.2171, -4.2171, -4.2171, -4.2171, -4.2171, -4.2171, -4.2171, -4.2171, -4.2171, -4.2171, -4.2171, -4.2171, -4.2171, -4.2171, -4.2171, -4.2171, -4.2171, -4.2171, -4.2171, -3.6107, -3.2362, -4.217, -4.2171, -3.361, -3.9671, -3.9671, -3.9671, -3.9671, -3.9671, -3.9671, -3.9671, -3.9671, -3.9671, -3.9671, -3.9671, -3.9671, -3.9671, -3.9671, -3.9671, -3.9671, -3.9671, -3.9671, -3.9671, -3.9671, -3.9671, -3.9671, -3.9671, -3.9672, -3.9672, -3.9672, -3.9672, -3.9674, -3.9674, -2.7368, -3.7176, -3.7176, -3.7176, -3.7176, -3.7176, -3.7176, -3.7176, -3.7176, -3.7176, -3.7176, -3.1114, -3.7176, -3.7176, -5.5093, -5.5093, -5.5094, -5.5094, -5.5094, -5.5094, -5.5094, -5.5094, -5.5094, -5.5094, -5.5094, -5.5094, -5.5094, -5.5094, -5.5094, -5.5094, -5.5093, -5.5093, -5.5094, -5.5094, -5.5094, -5.5094, -5.5094, -5.5094, -5.5094, -5.5094, -5.5094, -5.5094, -5.5094, -5.5094], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.9671, 0.9227, 0.8993, 0.8623, 0.8623, 0.8623, 0.7952, 0.7952, 0.7952, 0.7952, 0.7952, 0.7952, 0.7952, 0.7952, 0.7952, 0.7952, 0.7952, 0.7952, 0.7952, 0.663, 0.6357, 0.6357, 0.6357, 0.6357, 0.6357, 0.6357, 0.6357, 0.6357, 0.6357, 0.6357, 0.5138, 0.3193, 0.3193, 1.2733, 1.2733, 1.2053, 1.1304, 1.1304, 1.1304, 1.1304, 1.1304, 0.9545, 0.9545, 0.9545, 0.9545, 0.9545, 0.9545, 0.9545, 0.9545, 0.9545, 0.9545, 0.9545, 0.9545, 0.9545, 0.9545, 0.9545, 0.9545, 0.9545, 0.9545, 0.9545, 0.9545, 0.9545, 0.9545, 0.8508, 0.8508, 0.4532, 0.4872, 1.4498, 1.4045, 1.3232, 1.3232, 1.3232, 1.3232, 1.1343, 1.1343, 1.1343, 1.1343, 1.1343, 1.1343, 1.1343, 1.1343, 1.1343, 1.1343, 1.1343, 1.1343, 1.1343, 1.1343, 1.1343, 1.1343, 1.1343, 1.1343, 1.1343, 1.1343, 1.1343, 1.1343, 1.1343, 0.9802, 0.9656, 0.651, 0.6891, 1.7243, 1.4967, 1.4967, 1.4967, 1.4967, 1.4967, 1.4967, 1.4967, 1.4967, 1.4967, 1.4967, 1.4967, 1.4967, 1.4967, 1.4967, 1.4967, 1.4967, 1.4967, 1.4967, 1.4967, 1.4967, 1.4967, 1.4967, 1.4967, 1.4967, 1.4967, 1.2068, 1.2068, 0.9684, 0.9684, 2.2896, 1.8785, 1.8785, 1.8785, 1.8785, 1.8785, 1.8785, 1.8785, 1.8785, 1.8785, 1.8785, 1.7693, 1.4563, 1.4563, -0.0455, -0.0455, -0.0455, -0.0455, -0.0455, -0.0455, -0.0455, -0.0455, -0.0455, -0.0455, -0.0455, -0.0455, -0.0455, -0.0455, -0.0455, -0.0455, -0.5735, -0.5735, -0.4241, -0.6414, -0.6677, -0.9185, -1.2105, -0.985, -0.6033, -0.9102, -0.9102, -0.2569, -0.2569, -0.2569]}, \"token.table\": {\"Topic\": [3, 3, 1, 3, 1, 2, 2, 1, 3, 1, 3, 2, 4, 2, 3, 2, 2, 2, 4, 4, 1, 1, 3, 1, 3, 1, 4, 2, 2, 1, 1, 2, 3, 3, 3, 2, 4, 2, 1, 3, 4, 3, 4, 1, 1, 4, 3, 3, 2, 3, 3, 3, 3, 1, 2, 2, 3, 2, 1, 4, 4, 1, 4, 4, 1, 1, 1, 1, 1, 4, 3, 3, 4, 4, 2, 3, 3, 3, 2, 1, 2, 1, 2, 2, 4, 2, 4, 3, 1, 1, 1, 1, 2, 3, 4, 2, 2, 4, 2, 1, 4, 4, 5, 2, 3, 4, 4, 1, 1, 3, 5, 1, 4, 2, 2, 1, 4, 1, 1, 2, 1, 4, 4, 2, 2, 2, 3, 3, 3, 2, 4, 2, 2, 3, 1, 1, 3, 4, 4, 1, 1, 3, 3, 3], \"Freq\": [0.8538012265080364, 0.8538012265080364, 0.773448056333046, 0.8538012265080364, 0.773448056333046, 0.8184818310400261, 0.8184818310400261, 0.526542033068013, 0.526542033068013, 0.596093877644635, 0.2980469388223175, 0.8184818310400261, 0.9554946531353842, 0.4024296312612082, 0.4024296312612082, 0.8184818310400261, 0.8184818310400261, 0.779397206050121, 0.9554946531353842, 0.9554946531353842, 0.7734482308029652, 0.9896610445120685, 0.8538012265080364, 0.9896609240474137, 0.8538012265080364, 0.5634991963386132, 0.5634991963386132, 0.8184818310400261, 0.8184818310400261, 0.7734482308029652, 0.7469029530690388, 0.3734514765345194, 0.8538012265080364, 0.8538012265080364, 0.8538012265080364, 0.8184818310400261, 0.9554946531353842, 0.8184818310400261, 0.39913015810213914, 0.39913015810213914, 0.9554946531353842, 0.8538012265080364, 0.9554946531353842, 0.7275702893001362, 0.7275702546337266, 0.9554946531353842, 0.8538012265080364, 0.8538012265080364, 0.2704541627123435, 0.540908325424687, 0.5625788348888324, 0.5625788348888324, 0.8538012265080364, 0.5128909461883313, 0.5128909461883313, 0.788813873231093, 1.003304788338575, 0.8184818310400261, 0.7734482308029652, 0.9554946531353842, 0.9554946531353842, 0.5634991963386132, 0.5634991963386132, 0.6543580156932691, 0.9896610445120685, 0.9896610445120685, 0.9896610445120685, 0.9896609240474137, 0.9512854537711276, 0.9554946531353842, 0.8538012265080364, 0.8538012265080364, 0.9554946531353842, 0.9554946531353842, 0.5323074629930773, 0.5625788348888324, 0.8538012265080364, 0.8538012265080364, 0.8184818310400261, 0.9896610445120685, 0.8184818310400261, 0.8628484506668944, 0.8184818310400261, 0.5323074629930773, 0.9554946531353842, 0.8184818310400261, 0.9554946531353842, 0.8538012265080364, 0.773448056333046, 0.773448056333046, 0.773448056333046, 0.773448056333046, 0.8184818310400261, 0.8538012265080364, 0.9554946531353842, 0.8184818310400261, 0.8184818310400261, 0.9554946531353842, 0.8184818310400261, 0.773448056333046, 0.9554946531353842, 0.9554942990897705, 0.6169434699877588, 0.5470278889537186, 0.5470278889537186, 0.9554946531353842, 0.9554946531353842, 0.773448056333046, 0.9896610445120685, 0.5332945222380743, 0.5332945222380743, 0.9896609240474137, 0.9554946531353842, 0.5323074629930773, 0.8184818310400261, 0.773448056333046, 0.9554946531353842, 0.9896609240474137, 0.773448056333046, 0.8184818310400261, 0.983593812091884, 0.9554946531353842, 0.9554946531353842, 0.8184818310400261, 0.5323074629930773, 0.8184818310400261, 0.8538012265080364, 0.8538012265080364, 0.8389882774099789, 0.5323074629930773, 0.7150226236336639, 0.7793972039668694, 0.4024296312612082, 0.4024296312612082, 0.9896609240474137, 0.9896609240474137, 0.8538012265080364, 0.9554942990897705, 0.7150226236336639, 0.7275702893001362, 0.9896609240474137, 0.8538012265080364, 0.8538012265080364, 0.5625788348888324], \"Term\": [\".bz2\", \".gz\", \".reshap\", \"3k\", \"__setitem__\", \"ad\", \"also\", \"argument\", \"argument\", \"array\", \"array\", \"asv\", \"bad\", \"becaus\", \"becaus\", \"befor\", \"bench-compar\", \"benchmark\", \"benefit\", \"better\", \"bit\", \"broadcast\", \"byte\", \"checker\", \"clarifi\", \"code\", \"code\", \"command\", \"complet\", \"correct\", \"could\", \"could\", \"current\", \"decod\", \"decompress\", \"develop\", \"difficult\", \"doc\", \"docstr\", \"docstr\", \"editors/develop\", \"encod\", \"environ\", \"error\", \"exampl\", \"experi\", \"extens\", \"fact\", \"file\", \"file\", \"filenam\", \"first\", \"fname\", \"follow\", \"follow\", \"function\", \"generat\", \"get\", \"give\", \"haystack\", \"hit\", \"import\", \"import\", \"improv\", \"incorrect\", \"ind\", \"index\", \"initi\", \"int\", \"interest\", \"issu\", \"iter\", \"jupyt\", \"know\", \"lie\", \"loadtxt\", \"make\", \"may\", \"mayb\", \"messag\", \"minut\", \"mismatch\", \"much\", \"multipli\", \"needl\", \"new\", \"notebook\", \"np.loadtxt\", \"np.prod\", \"numpy.arang\", \"numpy.array\", \"numpy.zero\", \"optim\", \"patch\", \"perfect\", \"perform\", \"pip\", \"play\", \"preinstal\", \"produc\", \"propos\", \"qualiti\", \"rais\", \"read\", \"read\", \"rememb\", \"renam\", \"request\", \"result\", \"return\", \"return\", \"rng\", \"role\", \"run\", \"runtests.pi\", \"say\", \"searchsort\", \"seed\", \"set\", \"setup\", \"shape\", \"shift-tab\", \"signatur\", \"sin/cos/exp\", \"singl\", \"spend\", \"state\", \"str\", \"string\", \"suit\", \"sure\", \"test\", \"time\", \"time\", \"tri\", \"type\", \"unicod\", \"unifi\", \"user\", \"valu\", \"warn\", \"well\", \"yet\", \"yield\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 4, 1, 2, 5]};\n\nfunction LDAvis_load_lib(url, callback){\n  var s = document.createElement('script');\n  s.src = url;\n  s.async = true;\n  s.onreadystatechange = s.onload = callback;\n  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n  document.getElementsByTagName(\"head\")[0].appendChild(s);\n}\n\nif(typeof(LDAvis) !== \"undefined\"){\n   // already loaded: just create the visualization\n   !function(LDAvis){\n       new LDAvis(\"#\" + \"ldavis_el801398433851731526551792193\", ldavis_el801398433851731526551792193_data);\n   }(LDAvis);\n}else if(typeof define === \"function\" && define.amd){\n   // require.js is available: use it to load d3/LDAvis\n   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n   require([\"d3\"], function(d3){\n      window.d3 = d3;\n      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n        new LDAvis(\"#\" + \"ldavis_el801398433851731526551792193\", ldavis_el801398433851731526551792193_data);\n      });\n    });\n}else{\n    // require.js not available: dynamically load d3 & LDAvis\n    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n                 new LDAvis(\"#\" + \"ldavis_el801398433851731526551792193\", ldavis_el801398433851731526551792193_data);\n            })\n         });\n}\n</script>"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"2ff4a079f0da47ee9afdcd211e72829d","source_hash":"f1679362","execution_start":1683276140514,"execution_millis":273,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"from gensim.models.coherencemodel import CoherenceModel\ncoherence_model_lda = CoherenceModel(model=lda, texts=texts, dictionary=dictionary, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)#0.5588166880236373","execution_count":23,"outputs":[{"name":"stdout","text":"\nCoherence Score:  0.7486601758791229\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=4121e9a6-526b-4eeb-ae8d-65c5504611ef' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"47146847986a4dffb0a4100b7d4448a5","deepnote_execution_queue":[],"deepnote_persisted_session":{"createdAt":"2023-05-05T09:30:07.487Z"}}}